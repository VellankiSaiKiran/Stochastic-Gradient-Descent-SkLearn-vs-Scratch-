# Stochastic Gradient Descent: SKLearn vs Scratch

This project explores the implementation and performance of the Stochastic Gradient Descent (SGD) algorithm for machine learning. It compares the efficiency, accuracy, and runtime of SGD implementations using SKLearn's built-in functions versus a custom-built algorithm from scratch.

## Overview

Stochastic Gradient Descent is a fundamental optimization algorithm used in machine learning to minimize a function by iteratively moving towards the minimum value of the gradient. This project focuses on comparing two approaches to SGD:
- **SKLearn Implementation**: Utilizing the SGD functionalities provided by the Scikit-Learn library.
- **Custom Implementation**: Developing an SGD algorithm from scratch to understand the underlying mechanics and to customize its behavior.

## Objectives

- To understand the principles behind stochastic gradient descent and its application in machine learning.
- To implement SGD from scratch and compare its performance against the SKLearn's implementation.
- To evaluate both implementations in terms of accuracy, speed, and scalability.

## Installation

Ensure you have the following Python libraries installed to run this project:

- NumPy
- Pandas
- Matplotlib
- Scikit-Learn

You can install these packages using pip:

```bash
pip install numpy pandas matplotlib scikit-learn
